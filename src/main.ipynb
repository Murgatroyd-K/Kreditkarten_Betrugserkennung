{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98f2e16-85f0-4c02-a43c-8ca1be00a8ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importieren der erforderlichen Bibliotheken und Module für Datenverarbeitung und maschinelles Lernen\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RobustScaler,VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors,DenseVector\n",
    "from pyspark.sql.functions import col,max,min,udf, concat\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,GBTClassifier,LinearSVC\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14719caa-57e3-4289-a5e2-560113d86395",
     "showTitle": true,
     "title": "Hilfsfunktionen"
    }
   },
   "outputs": [],
   "source": [
    "# Hilfsfunktion zum Extrahieren von Werten aus einem Vektor\n",
    "def extract_from_vector(vec):\n",
    "    return float(vec[0])\n",
    "\n",
    "# Funktion zur Analyse der Modellvorhersagen\n",
    "def analysis(predictions,model):\n",
    "    # Umwandeln der Vorhersagen in ein RDD-Format für die Metriken-Berechnung\n",
    "    predictionAndLabels = predictions.rdd.map(lambda lp: (float(lp.prediction), float(lp.Class)))\n",
    "    \n",
    "    # Berechnen und Ausgeben von Precision, Recall und F1-Wert für jedes Label    \n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    # Erstllen einer Liste für Rückgabewerte\n",
    "    list_dict_return= []\n",
    "    # Berechnen der Genauigkeit für jedes Label\n",
    "    labels = predictionAndLabels.map(lambda lp: lp[1]).distinct().collect()\n",
    "    for label in sorted(labels):\n",
    "        precision = metrics.precision(label)\n",
    "        recall = metrics.recall(label)\n",
    "        f1Score = metrics.fMeasure(label)\n",
    "        print(f\"Label {label}: Precision = {precision}, Recall = {recall}, F1 Score = {f1Score}\")\n",
    "        list_dict_return.append({\"Label\":label, \"Precision\":precision,\"Recall\":recall, \"F1 Score\":f1Score, \"Model\":model})\n",
    "    return(list_dict_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad83a98d-950e-4b65-aa79-c77311a274dd",
     "showTitle": true,
     "title": "Daten Laden"
    }
   },
   "outputs": [],
   "source": [
    "# Pfad und Typ der zu ladenden Datei\n",
    "file_location = \"/FileStore/tables/creditcard.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# Optionen für das Einlesen der CSV-Datei\n",
    "infer_schema = \"True\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Laden der CSV-Datei als DataFrame\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "# Konvertieren aller Spalten in DoubleType für die maschinelle Verarbeitung\n",
    "columns = df.columns\n",
    "for column in columns:\n",
    "    df = df.withColumn(column, col(column).cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbb5a88-c694-4206-b08f-0389009b06d0",
     "showTitle": true,
     "title": "scaling amount"
    }
   },
   "outputs": [],
   "source": [
    "# Erstellen eines VectorAssembler-Objekts zur Transformation der Daten in ein für ML-Modelle geeignetes Format\n",
    "assembler = VectorAssembler(inputCols=[\"Amount\"], outputCol=\"v_amount\")\n",
    "scaler = RobustScaler(inputCol=\"v_amount\", outputCol=\"scaledFeatures\", \n",
    "                      withScaling=True, withCentering=False,\n",
    "                      lower=0.25, upper=0.75)\n",
    "\n",
    "# Transformieren des DataFrames\n",
    "transformed_df = assembler.transform(df)\n",
    "\n",
    "# Fit und Transformation des DataFrames\n",
    "scalerModel = scaler.fit(transformed_df)\n",
    "scaledData = scalerModel.transform(transformed_df)\n",
    "extract_udf = udf(extract_from_vector, DoubleType())\n",
    "df_with_extracted_value = scaledData.withColumn(\"Scal_Amount\", extract_udf(scaledData[\"scaledFeatures\"]))\n",
    "df = df_with_extracted_value.drop(\"scaledFeatures\",\"v_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78aceb3-5ee4-4a08-b61f-417fd1210b90",
     "showTitle": true,
     "title": "scaling time"
    }
   },
   "outputs": [],
   "source": [
    "min_time = df.select(min(col(\"Time\"))).first()[0]\n",
    "max_time = df.select(max(col(\"Time\"))).first()[0]\n",
    "df = df.withColumn(\"Scal_Time\", (col(\"Time\") - min_time) / (max_time - min_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c187e8-99cc-4922-afbc-f9b25efa7c49",
     "showTitle": true,
     "title": "Ergebnis vom scaling"
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"Amount\").describe().show()\n",
    "df.select(\"Scal_Amount\").describe().show()\n",
    "df.select(\"Time\").describe().show()\n",
    "df.select(\"Scal_Time\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16d10db-b9c9-4c1a-b2d4-27f13e5f8567",
     "showTitle": true,
     "title": "Feature Vektor erstellen und Daten splitten "
    }
   },
   "outputs": [],
   "source": [
    "# Definition der zu verwendenden Spalten\n",
    "feature_columns = df.columns\n",
    "feature_columns.remove(\"Amount\")\n",
    "feature_columns.remove(\"Time\")\n",
    "feature_columns.remove(\"Class\")\n",
    "label_column = \"Class\"\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_t = vector_assembler.transform(df)\n",
    "# Aufteilung der Daten in Trainings- und Testdatensätze\n",
    "train_data, test_data, validation_data = df_t.randomSplit([0.80, 0.10, 0.10], seed=1)\n",
    "list_analysis = []\n",
    "list_analysis_down =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "537ef9a9-341c-404f-be2d-94811147880e",
     "showTitle": true,
     "title": "LogisticRegression"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=label_column)\n",
    "model = lr.fit(train_data)\n",
    "predictions = model.transform(validation_data)\n",
    "list_analysis.extend(analysis(predictions,\"lr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a06654-0c94-4582-910c-e8658f9b33ec",
     "showTitle": true,
     "title": "Randomforest"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=label_column, featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(train_data)\n",
    "predictions = model.transform(validation_data)\n",
    "list_analysis.extend(analysis(predictions,\"rf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239a1503-70c0-4028-88ea-b6dd3305f400",
     "showTitle": true,
     "title": "Gradient Boosting"
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=label_column, featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(train_data)\n",
    "predictions = model.transform(validation_data)\n",
    "list_analysis.extend(analysis(predictions,\"gbt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b951fe51-d05a-4d4e-a6c3-45f6409ef095",
     "showTitle": true,
     "title": " Linear Support Vector Classifier "
    }
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol=label_column, featuresCol=\"features\")\n",
    "model = lsvc.fit(train_data)\n",
    "predictions = model.transform(validation_data)\n",
    "list_analysis.extend(analysis(predictions,\"lsvc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aca4051-bdff-4934-a6b6-9f208cb17cd1",
     "showTitle": true,
     "title": "Downsampling "
    }
   },
   "outputs": [],
   "source": [
    "class_1_df = df_t.filter(col(\"class\") == 1)\n",
    "\n",
    "class_1_count = class_1_df.count()\n",
    "\n",
    "class_0_count = df_t.filter(col(\"class\") == 0).count()\n",
    "sample_fraction = class_1_count / float(class_0_count)\n",
    "class_0_sampled_df = df_t.filter(col(\"class\") == 0).sample(withReplacement=False, fraction=sample_fraction)\n",
    "balanced_df = class_1_df.union(class_0_sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99dfd7dd-cfe0-4443-9861-c864b071f2a0",
     "showTitle": true,
     "title": "Downsampling split"
    }
   },
   "outputs": [],
   "source": [
    "train_data_b, test_data_b, validation_data_b = balanced_df.randomSplit([0.80, 0.10, 0.10], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68078dc0-25b8-4af3-b0be-14b0010fb3ea",
     "showTitle": true,
     "title": "LogisticRegression"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=label_column)\n",
    "model = lr.fit(train_data_b)\n",
    "predictions = model.transform(validation_data_b)\n",
    "list_analysis_down.extend(analysis(predictions,\"lr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527d2327-4fb3-4b0c-aa48-d888385643b8",
     "showTitle": true,
     "title": "Randomforest"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=label_column, featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(train_data_b)\n",
    "predictions = model.transform(validation_data_b)\n",
    "list_analysis_down.extend(analysis(predictions,\"rf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95df9b10-7c06-4188-822e-4e9f646c6eb1",
     "showTitle": true,
     "title": "Gradient Boosting"
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=label_column, featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(train_data_b)\n",
    "predictions = model.transform(validation_data_b)\n",
    "list_analysis_down.extend(analysis(predictions,\"gbt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c58cbdf-29f9-4cf7-9ead-45dae241f16f",
     "showTitle": true,
     "title": " Linear Support Vector Classifier "
    }
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol=label_column, featuresCol=\"features\")\n",
    "model = lsvc.fit(train_data_b)\n",
    "predictions = model.transform(validation_data_b)\n",
    "list_analysis_down.extend(analysis(predictions,\"lsvc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1226e177-8603-425c-9a88-72260e229c19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(list_analysis)\n",
    "df_result = spark.createDataFrame(rdd)\n",
    "df_result = df_result.withColumn(\"Model_label\", concat(df_result.Model, df_result.Label))\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e686f9c-d604-4295-b69b-e0b8db026f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(list_analysis_down)\n",
    "df_result_down = spark.createDataFrame(rdd)\n",
    "df_result_down = df_result_down.withColumn(\"Model_label\", concat(df_result_down.Model, df_result_down.Label))\n",
    "display(df_result_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a555c05-439b-405f-919f-18d76fe1b69e",
     "showTitle": true,
     "title": "Ergebnis"
    }
   },
   "outputs": [],
   "source": [
    "rf = GBTClassifier(labelCol=label_column, featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "list_analysis_down.extend(analysis(predictions,\"gbt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0a1471-8e98-4f3c-965e-fbf720d18e55",
     "showTitle": true,
     "title": "Ergebnis Down"
    }
   },
   "outputs": [],
   "source": [
    "rf = GBTClassifier(labelCol=label_column, featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(train_data_b)\n",
    "predictions = model.transform(test_data)\n",
    "list_analysis_down.extend(analysis(predictions,\"gbt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87bdf4e8-dec4-4efd-98e7-6c7f45482c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "a2843eee-d4e9-424e-89ff-5232e4b8d4ed",
     "origId": 2854789446853587,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    },
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "c0db1ede-dd88-4d16-8786-cfecf476fc0e",
     "origId": 2854789446853588,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    },
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "69440768-30ef-4b0c-9110-185169534efc",
     "origId": 2854789446853589,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
